layer {
	name: "Input_data"
	type: "Input"
	top: "input_data"
	input_param {
		shape {
			dim: 1 #batch_size
			dim: 100
            dim: 41
            dim: 41
		}
	}
}

layer {
	name: "Input_value"
	type: "Input"
	top: "label_value"
	input_param {
		shape {
			dim: 1 #batch_size
			dim: 1
		}
	}
	include {
		phase: TRAIN
	}
}

layer {
	name: "Input_probabs"
	type: "Input"
	top: "label_probabs"
	input_param {
		shape {
			dim: 1 #batch_size
			dim: 67240
		}
	}
	include {
		phase: TRAIN
	}
}

layer {
    name: "conv1"
    type: "Convolution"
    bottom: "input_data"
    top: "conv1"
    convolution_param {
        num_output: 256
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "gaussian"
            std: 0.01
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}

layer {
	name: "conv1_relu"
	type: "ReLU"
	bottom: "conv1"
	top: "conv1"
    relu_param {
        negative_slope: 0.02
    }
}

#Start of residual block 1
layer {
    name: "res1_conv1"
    type: "Convolution"
    bottom: "conv1"
    top: "res1_conv1"
    convolution_param {
        num_output: 256
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "gaussian"
            std: 0.01
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}

layer {
	name: "res1_relu1"
	type: "ReLU"
	bottom: "res1_conv1"
	top: "res1_conv1"
    relu_param {
        negative_slope: 0.02
    }
}

layer {
    name: "res1_conv2"
    type: "Convolution"
    bottom: "res1_conv1"
    top: "res1_conv2"
    convolution_param {
        num_output: 256
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "gaussian"
            std: 0.01
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}

layer {
	name: "res1_sum"
	type: "Eltwise"
	bottom: "conv1"
	bottom: "res1_conv2"
	top: "res1"
	eltwise_param {
		operation: SUM
	}
}

layer {
	name: "res1_relu2"
	type: "ReLU"
	bottom: "res1"
	top: "res1"
    relu_param {
        negative_slope: 0.02
    }
}

#Start of residual block 40
layer {
    name: "res40_conv1"
    type: "Convolution"
    bottom: "res1"
    top: "res40_conv1"
    convolution_param {
        num_output: 256
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "gaussian"
            std: 0.01
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}

layer {
        name: "res40_relu1"
        type: "ReLU"
        bottom: "res40_conv1"
        top: "res40_conv1"
    relu_param {
        negative_slope: 0.02
    }
}

layer {
    name: "res40_conv2"
    type: "Convolution"
    bottom: "res40_conv1"
    top: "res40_conv2"
    convolution_param {
        num_output: 256
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "gaussian"
            std: 0.01
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}

layer {
        name: "res40_sum"
        type: "Eltwise"
        bottom: "res39"
        bottom: "res40_conv2"
        top: "res40"
        eltwise_param {
                operation: SUM
        }
}

layer {
        name: "res40_relu2"
        type: "ReLU"
        bottom: "res40"
        top: "res40"
    relu_param {
        negative_slope: 0.02
    }
}


### Value head ###

layer {
    name: "Value_conv1"
    type: "Convolution"
    bottom: "res40"
    top: "value_conv1"
    convolution_param {
        num_output: 2
        kernel_size: 1
        stride: 1
        pad: 0
        weight_filler {
            type: "gaussian"
            std: 0.01
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}

layer {
	name: "value_relu1"
	type: "ReLU"
	bottom: "value_conv1"
	top: "value_conv1"
    relu_param {
        negative_slope: 0.02
    }
}

layer {
	name: "Value_fc1"
	type: "InnerProduct"
	bottom: "value_conv1"
	top: "value_fc1"
	inner_product_param {
		num_output: 8
		weight_filler {
            type: "gaussian"
            std: 0.01
		}
		bias_filler {
			type: "constant"
			value: 0
		}
	}
}

layer {
	name: "relu_value_fc1"
	type: "ReLU"
	bottom: "value_fc1"
	top: "value_fc1"
    relu_param {
        negative_slope: 0.02
    }
}

layer {
	name: "Value_Output"
	type: "InnerProduct"
	bottom: "value_fc1"
	top: "output_value"
	inner_product_param {
		num_output: 1
		weight_filler {
            type: "gaussian"
            std: 0.01
		}
		bias_filler {
			type: "constant"
			value: 0
		}
	}
}

layer {
	name: "TanH_value"
	type: "TanH"
	bottom: "output_value"
	top: "output_value"
}

layer {
	name: "Value_loss"
	type: "EuclideanLoss"
	bottom: "output_value"
	bottom: "label_value"
	top: "value_loss"
	include {
		phase: TRAIN
	}
}

############## probabs head ################

layer {
    name: "probabs_conv1"
    type: "Convolution"
    bottom: "res40"
    top: "probabs_conv1"
    convolution_param {
        num_output: 2
        kernel_size: 1
        stride: 1
        pad: 0
        weight_filler {
            type: "gaussian"
            std: 0.01
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}

layer {
	name: "probabs_relu1"
	type: "ReLU"
	bottom: "probabs_conv1"
	top: "probabs_conv1"
    relu_param {
        negative_slope: 0.02
    }
}

layer {
	name: "probabs_Output"
	type: "InnerProduct"
	bottom: "probabs_conv1"
	top: "raw_output_probabs"
	inner_product_param {
		num_output: 67240
		weight_filler {
            type: "gaussian"
            std: 0.01
		}
		bias_filler {
			type: "constant"
			value: 0
		}
	}
}

layer {
	name: "probabs_Softmax"
	type: "Softmax"
	bottom: "raw_output_probabs"
	top: "output_probabs"
}

### As Caffe doesn't have a cross entropy loss layer, we create it from existing layers ###
layer {
	name: "Log_probabs"
	type: "Log"
	bottom: "output_probabs"
	top: "log_output_probabs"
	include {
		phase: TRAIN
	}
}

layer {
	name: "Minus_Log_probabs"
	type: "Scale"
	bottom: "log_output_probabs"
	top: "minus_log_probabs"
	param {
		lr_mult: 0
		decay_mult: 0
	}
	scale_param {
        axis: 0
		num_axes: 0
		filler {
			type: "constant"
			value: -1
		}
		bias_term: false
	}
	include {
		phase: TRAIN
	}
}

layer {
	name: "Cross_Entropy_Mult"
	type: "Eltwise"
	bottom: "minus_log_probabs"
	bottom: "label_probabs"
	top: "eltwise_prod"
	eltwise_param {
		operation: PROD
	}
	include {
		phase: TRAIN
	}
}

layer {
	name: "Cross_entropy_first_sum"
	type: "Reduction"
	bottom: "eltwise_prod"
	top: "cross_entropy_summed"
	reduction_param {
		operation: SUM
		axis: 1
	}
	include {
		phase: TRAIN
	}
}

layer {
	name: "Cross_Entropy_Scale"
	type: "Scale"
	bottom: "cross_entropy_summed"
	top: "cross_entropy_scaled"
	param {
		lr_mult: 0
		decay_mult: 0
	}
	scale_param {
        axis: 0
		num_axes: 0
		filler {
			type: "constant"
			value: 7 #1 / batch_size
		}
		bias_term: false
	}
	include {
		phase: TRAIN
	}
}

layer {
	name: "Cross_entropy_loss"
	type: "Reduction"
	bottom: "cross_entropy_scaled"
	top: "probabs_loss"
	reduction_param {
		operation: SUM
		axis: 0
	}
	include {
		phase: TRAIN
	}
    loss_weight: 1
}

